{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Game Theory I: Normal Form\n",
    "======================\n",
    "\n",
    "Shane Steinert-Threlkeld  \n",
    "http://www.shane.st  \n",
    "S.N.M.Steinert-Threlkeld AT uva DOT nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       ".rendered_html tbody tr td:first-child {\n",
       "    border-right: 1px solid black;\n",
       "}\n",
       "    \n",
       ".rendered_html table {\n",
       "    font-size: 28px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style type=\"text/css\">\n",
    ".rendered_html tbody tr td:first-child {\n",
    "    border-right: 1px solid black;\n",
    "}\n",
    "    \n",
    ".rendered_html table {\n",
    "    font-size: 28px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is game theory?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![AlphaGo Nature paper](imgs/AlphaGo_cover.jpg)\n",
    "\n",
    "credit: Nature Publishing Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Robben in the 2014 World Cup quarter-finals](imgs/robben_shootout.png)\n",
    "\n",
    "credit: EPA/Ali Haider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://giphy.com/embed/uH27sheFg9One\" width=\"600\" height=\"360\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe><p><a href=\"https://giphy.com/gifs/stanley-kubrick-1964-dr-strangelove-uH27sheFg9One\">via GIPHY</a></p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://giphy.com/embed/uH27sheFg9One\" width=\"600\" height=\"360\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe><p><a href=\"https://giphy.com/gifs/stanley-kubrick-1964-dr-strangelove-uH27sheFg9One\">via GIPHY</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![One of Google's self-driving cars](imgs/google_car.jpg)\n",
    "\n",
    "Source: Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is game theory?\n",
    "------\n",
    "\n",
    "* The study of strategic interaction.\n",
    "* Essentially: _multi-agent decision theory_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Shootout\n",
    "------\n",
    "\n",
    "![Robben in the 2014 World Cup quarter-finals](imgs/robben_shootout.png)\n",
    "\n",
    "credit: EPA/Ali Haider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Shootout in Normal Form\n",
    "-------\n",
    "\n",
    "| &#160; | dive left | dive right |\n",
    "| ----- | ----- | ----- |\n",
    "| shoot left | 1, -1 | -1, 1 |\n",
    "| shoot right | -1, 1 | 1, -1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Definition of a (Normal-Form) Game\n",
    "------\n",
    "\n",
    "A finite, $n$-person _normal form game_ (aka strategic form) is a tuple $\\langle N, \\{ A_i \\}, \\{ u_i \\} \\rangle$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $N$ is the number of players.  We represent them as integers $1, \\dots , n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $A_i$ is the set of _actions_ available to player $i$.\n",
    "\n",
    "    Write $A = A_1 \\times \\cdots \\times A_n$ for the set of _action profiles_: a specification of on action for each player.\n",
    "    \n",
    "    For reasons that will become clear, we will also call actions _pure strategies_ (and mutatis mutandis for profiles thereof).  We will also use $S_i$ and $S$ for the sets of strategies and profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $u_i : A \\to \\mathbb{R}$ is player $i$'s utility function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Multi-Agent Decision Theory\n",
    "-----\n",
    "\n",
    "Note the differences between a decision problem and a game:\n",
    "\n",
    "* No states.  \n",
    "\n",
    "    Utilities are defined over action profiles.  Source of uncertainty: what will the other agents do?\n",
    "    \n",
    "* No probabilities.\n",
    "\n",
    "    How do we apply tools like `maximize_expected_utility`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Common Knowledge of Rationality\n",
    "-------\n",
    "\n",
    "* Common knowledge that $p$: We all know $p$, we know that we know $p$, we know that we know that we know $p$, ...\n",
    "    - Inifinite iteration: not needed in many cases; useful for normative evaluation\n",
    "* Generally, the following are assumed to be common knowledge among players of a game:\n",
    "    - the precise definition of the game (acts, utilities)\n",
    "    - _that everyone is rational_ (e.g. an expected utility maximizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mutually Assured Destruction (Prisoner's Dilemma)\n",
    "------\n",
    "\n",
    "\n",
    "| &#160; | disarm | arm |\n",
    "| ----- | ----- | ----- |\n",
    "| disarm | 0, 0 | -10, 10 |\n",
    "| arm | 10, -10 | -8, -8 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# we define an n-player game as an n+1-dimensional array\n",
    "# dimensions 0 to n-1 correspond to the actions available to each agent\n",
    "# dimension n gives the utilities for each agent of the corresponding action profile\n",
    "MAD = np.array([\n",
    "    # row 1\n",
    "    [[0, 0], [-10, 10]],\n",
    "    # row 2\n",
    "    [[10, -10], [-8, -8]]\n",
    "])\n",
    "\n",
    "def action_profiles(game):\n",
    "    # last dimension is # of players, so we ignore it\n",
    "    return itertools.product(*[range(num_actions) for num_actions in game.shape[:-1]])\n",
    "\n",
    "print(list(action_profiles(MAD)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Solution Concepts, I: Pareto Efficiency\n",
    "--------\n",
    "\n",
    "Intuitively: any change in strategies would make _someone_ worse off.\n",
    "\n",
    "* $s$ Pareto dominates $s'$ iff: $u_i(s) \\geq u_i(s')$ for every $i$, and $u_i(s) > u_i(s')$ for some $i$\n",
    "* $s$ is Pareto optimal iff no strategy profile $s'$ Pareto dominates $s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 0)]\n"
     ]
    }
   ],
   "source": [
    "def pareto_dominates(game, profile1, profile2):\n",
    "    return np.all(game[profile1] >= game[profile2]) and np.any(game[profile1] > game[profile2])\n",
    "\n",
    "def pareto_optimal(game):\n",
    "    profiles = list(action_profiles(game))\n",
    "    for profile in profiles:\n",
    "        if not any(pareto_dominates(game, alt_profile, profile) \n",
    "                   for alt_profile in profiles):\n",
    "            yield profile\n",
    "\n",
    "print(list(pareto_optimal(MAD)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In other words: (disarm, disarm) is Pareto optimal.  So, too, are (disarm, arm) and (arm, disarm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Solution Concepts, II: Iterated Dominance\n",
    "-----\n",
    "\n",
    "* $s_1$ (strictly) dominates $s_2$ for player $i$ iff\n",
    "    $$u_i(s_1 , s_{-i}) > u_i(s_2, s_{-1})$$\n",
    "    for all strategy profiles $s$\n",
    "* $s_1$ weakly dominates $s_2$ for player $i$ iff\n",
    "    $$u_i(s_1 , s_{-i}) \\geq u_i(s_2, s_{-1})$$\n",
    "    for all strategy profiles $s$ and\n",
    "    $$u_i(s_1 , s_{-i}) > u_i(s_2, s_{-1})$$\n",
    "    for some $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Solution Concepts, II: Iterated Dominance\n",
    "-----\n",
    "\n",
    "Iterated Removal of (strictly/weakly) Dominated Strategies:\n",
    "\n",
    "* As long as there are dominated strategies in the game $G$:\n",
    "    - reduce the game by deleting the relevant strategy for the relevant player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Iterated Dominance in MAD\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: I have only tested this on two player games\n",
    "def get_utilities(game, player):\n",
    "    # gets a 1-D array of utilities for `player` for playing `strategy` in `game`\n",
    "    return game.take(player, axis=-1)\n",
    "\n",
    "def get_action_utilities(game, player, action):\n",
    "    return get_utilities(game, player).take(action, axis=player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "def strictly_dominates(game, player, action1, action2):\n",
    "    return np.all(get_action_utilities(game, player, action1) > \n",
    "                  get_action_utilities(game, player, action2))\n",
    "\n",
    "print(strictly_dominates(MAD, 0, 0, 1))\n",
    "# print(strictly_dominates(MAD, 0, 1, 0))\n",
    "# print(strictly_dominates(MAD, 1, 0, 1))\n",
    "# print(strictly_dominates(MAD, 1, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`arm` strictly dominates `disarm` for both players, so the only strategy profile that survives iterated removal is (arm, arm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Guessing Game\n",
    "-----\n",
    "\n",
    "> You all must guess a number between 1 and 100.  You win (and get utility $1$) if your guess is the closest to 2/3 of the average of everyone's guess.  Everyone else gets utility $0$. \n",
    "\n",
    "What is your guess?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Every number from 68-100 is weakly dominated by some number: 2/3 of the average is always less than or equal to 67.\n",
    "\n",
    "    So we can delete all choices 68-100 from the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Now, in the reduced game the highest possible average is 67; since 2/3 * 67 = 44 2/3, every number from 46 - 67 will be weakly dominated by some number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Now, the highest possible average is ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The only strategy to survive iterated removal of _weakly_ dominated strategies is: 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Solution Concepts III: Nash Equilibrium\n",
    "---------\n",
    "\n",
    "Key idea: no player wants to _unilaterally deviate_.  Given what the other players are doing, no one wants to change what they are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nash Equilibrium via Best Responses\n",
    "----------\n",
    "\n",
    "* $s_i$ is a best response to $s_{-i}$ (i.e. $s_i \\in \\text{BR}_i(s_{-i})$: \n",
    "$$\\text{BR}_i(s_{-i}) = \\{ s_i \\in S_i : u_i(s_i , s_{-i}) \\geq u_i(s_i^\\prime , s_{-i}) \\text{ for every } s_i^\\prime \\neq s_i \\in S_i \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* $s$ is a (weak) _Nash equilibrium_ if and only if for every agent $i$, $s_i \\in \\text{BR}_i(s_{-i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* strict Nash equilibrium: replace $\\geq$ with $>$ in the definition of best response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nash in MAD / PD\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* (disarm, disarm) is the only Nash equilibrium in the MAD game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This is something of a \"paradox\": individual rationality seems to inevitably lead to an outcome that is not Pareto optimal and is in fact strictly worse for all parties involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Back to the Penalty Kick\n",
    "----\n",
    "\n",
    "| &#160; | dive left | dive right |\n",
    "| ----- | ----- | ----- |\n",
    "| shoot left | 1, -1 | -1, 1 |\n",
    "| shoot right | -1, 1 | 1, -1 |\n",
    "\n",
    "What should Robben do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mixed Strategies\n",
    "-----\n",
    "\n",
    "* There are _no_ Nash equilibria in _pure_ strategies (i.e. no profile $(a_1, a_2)$ is a n.e., for any $a_1 \\in A_1$ and $a_2 \\in A_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The set of _mixed_ strategies for player $i$ is $$S_i := \\Delta(A_i)$$ where $\\Delta(\\cdot)$ is the set of probability distributions over a set.\n",
    "\n",
    "* The set of mixed strategy _profiles_ in a game is $S := S_1 \\times \\cdots \\times S_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Utility of a Mixed Strategy Profile\n",
    "-----\n",
    "\n",
    "\\begin{align*}\n",
    "u_i(s) &= \\mathbb{E}_{p_s}\\left[ u_i(a) \\right] \\\\\n",
    "&= \\sum_{a \\in A} u_i(a) p_s(a) \\\\\n",
    "&= \\sum_{a \\in A} u_i(a) \\prod_{j=1}^n s_j(a_j)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finding a Mixed Strategy NE\n",
    "------\n",
    "\n",
    "* Key idea: mix among your actions so as to make the other players _indifferent_ among their actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Suppose Robben kicks left with probability $p$ and kicks right with probability $1-p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "u_G(\\text{dive left}) &= p \\cdot -1 + (1-p) \\cdot 1 = 1 - 2p \\\\\n",
    "u_G(\\text{dive right}) &= p \\cdot 1 + (1-p) \\cdot -1 = 2p - 1 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Now: $u_G(\\text{dive left}) = u_G(\\text{dive right})$ if and only if $p = \\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Similar reasoning shows that Robben is indifferent about whether to kick left or right just in case the goalie mixes between his actions with probability $\\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* So: $( (1/2, 1/2), (1/2, 1/2))$ is the only Nash equilibrium of this game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nash's Theorem\n",
    "------\n",
    "\n",
    "**Theorem (Nash, 1951).** Every finite (in number of players and actions) normal-form game has at least one Nash equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Interpretations of Mixed Strategies\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Objective chance: when Robben is about to kick, it's as if he flips a coin and chooses which side to go based on that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Subjective probability: player $i$'s mixed strategy is _everyone else's_ degree of belief about what $i$ will do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Empirical frequency: how often strategies are played in the limit in _finitely repeated_ versions of the game "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Population frequency: what percentage of a _population_ plays each pure strategy  \n",
    "    Much more on this one next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Computing Nash Equilibria\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A Crucial Fact\n",
    "------\n",
    "\n",
    "**Theorem.** Let $s_1$ and $s_2$ be mixed strategies for the players in a 2-player game, and $A$ and $B$ their payoff matrices.  Then $s_1 \\in \\text{BR}_1(s_2)$ if and only if:\n",
    "$$\\text{if } i \\in \\text{supp}(s_1), \\text{ then } (As_2)_i = \\max\\{ (As_2)_k : k < | A_1 | \\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def is_nash(game, row_strategy, col_strategy):\n",
    "    # _strategy: 1-D numpy array; probability dist over their actions\n",
    "    row_utilities = get_utilities(game, 0)\n",
    "    col_utilities = get_utilities(game, 1)\n",
    "    \n",
    "    # get expected utilities\n",
    "    row_expected = np.dot(row_utilities, col_strategy)\n",
    "    col_expected = np.dot(col_utilities.T, row_strategy)\n",
    "    \n",
    "    # support = non-zero probability assigned\n",
    "    row_support = np.nonzero(row_strategy)\n",
    "    col_support = np.nonzero(col_strategy)\n",
    "    \n",
    "    return (row_expected.max() == row_expected[row_support].max() and\n",
    "            col_expected.max() == col_expected[col_support].max())\n",
    "\n",
    "print(is_nash(MAD, [0, 1], [1, 0]))\n",
    "print(is_nash(MAD, [0, 1], [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Support Enumeration Methods\n",
    "--------\n",
    "\n",
    "1. For every $k$ up to $\\min(|A_1|, |A_2|)$:\n",
    "    1. For every $I \\subset A_1, J \\subset A_2$ s.t. $|I| = |J| = k$:\n",
    "        1. Solve the system of equations for $p$ (and same for the other agent):\n",
    "        \\begin{align*}\n",
    "        &\\sum_{a_2 \\in J} p(a_2)u_1(a, a_2) = v && &\\forall a \\in I \\\\\n",
    "        &\\sum_{a_2 \\in J} p(a_2)u_1(a, a_2) \\leq v && &\\forall a \\notin I \\\\\n",
    "        &p(a) \\geq 0 && &\\forall a \\in I \\\\\n",
    "        &p(a) = 0 && &\\forall a \\notin I \\\\\n",
    "        &\\sum_{a \\in I} p(a) = 1\n",
    "        \\end{align*}\n",
    "        \n",
    "        \n",
    "See section 4.2.3 of Shohan and Leyton-Brown (linked on homepage of this repository) for more details, and the rest of that chapter for fancier algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wrapping up\n",
    "-----\n",
    "\n",
    "* Normal form games model _multiagent decision theory_ (so far, in simultaneous games)\n",
    "* Applies to a wide range of interactive contexts\n",
    "* Solution concepts: Pareto optimality, iterated removal of dominant strategies, Nash equilibrium\n",
    "    - Conflict between \"group\" and \"individual\" rationality\n",
    "* Next time: sequential structure, uncertainty about what game is being played"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
